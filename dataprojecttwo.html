<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Data Project 1</title>
    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="manifest" href="manifest.json">
    <link rel="mask-icon" href="safari-pinned-tab.svg" color="#00ff1c">
    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:300|Playfair+Display&display=swap">
    <link rel="stylesheet" type="text/css" href="./assets/css/twofourtyfive.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>
    <script src="./assets/js/main.js"></script>
  </head>
  <body>
    <div class="container-fluid main">
        <h1>Data Project 2: @mediabias, u up?</h1>
        <h2>About</h2>
        <p>Given the importance of Twitter in modern political discourse and the ever-polarizing topic of bias in media, I wanted to try to build a model to predict bias within tweets.</p>
        <h2>Data and Analysis</h2>
        <p>I collected data from 46 news/opinion Twitter accounts, covering the entire spectrum of political bias. In total, I analyzed 148,000 tweets from these sources.</p>
        <p>For each tweet, I cleaned, stemmed, and ran sentiment analysis to have consistent inputs for the models I later used to try to predict bias.</p>
        <p>Once the data was prepared, I wanted to first look at what words were used the most. Comparing frequency of appearance, TF-IDF frequency, and log-scaled frequency, it was obvious that certain words would be appear a lot more than others:</p>
        <br>
        <img />
        <br>
        <p>I ended up using TF-IDF frequency as my metric for choosing the vocabulary of my classifiers.</p>
        <img />
        <p>Once I choose this vocabulary, I one-hot encoded my training and test data with it and wanted to evaluate how a random forest would do at classifying the tweets. On the first run, I got only 33% accuracy, which was mildly disappointing.</p>
        <p>To improve accuracy, we have three main options:</p>
        <ul>
          <li><b>Gather more test data:</b> I didn't exactly want to do this, since my models were already taking long enough to run :)</li>
          <li><b>Tune hyperparameters:</b> This is a good place to start, seeing how we can adjust the model to work better with our data sounds appealing.</li>
          <li><b>Feature selection:</b> Spoiler alert, we'll come back to this!</li>
        </ul> 
        <p>I first decided to try tuning hyperparameters, namely, I wanted to look at how changing the number of trees and the maximum depth for them impacted accuracy on my training and test data. I first looked at the number of trees:</p>
        <img />
        <p>And found that training and test accuracy stayed pretty consistent past 100 trees. Max depth, on the other hand was a much more useful parameter to adjust:</p>
        <img />
        <p>Seeing that training accuracy increased linearly past depth 100 but training accuracy decreased, I knew that this was the reasonable limit of depth for my RF trees. However, at this point the accuracy was still lacking what one would reasonably hope for, so I decided to start trying to look into feature selection.</p>
        <p>By limiting the vocabulary of my classifiers, I knew that I could ensure that they were focused on learning real features of the data, and not just "memorizing mistakes". The first way I decided to go about this was by finding the words that the left-biased sources and right-biased sources use most differently, as measured by sentiment. This metric would also ideally filter out news-type words like 'reporting' or 'live' that would have no real indication of bias but were included because of a high frequency.</p>
        <img />
        <p>Running another RF with optimized parameters on this, accuracy stayed almost identical. This meant, in essence, that I was wrong to think that this feature alone would help my classifier learn less mistakes.</p>
        <p>I decided to next take a more stastical approach to determining importance, using singular value decomposition-based principal component analysis to determine which features were most important. At first glance, the PCA didn't seem to be very promising:</p>
        <img />
        <p>Seeing as the most important component only accounted for a fraction of a percent of variance. However, by using the first 500 most important components, we could directly account for up to 60%.</p>
        <img />
        <p>Running the random forest on the PCA-encoded dataset yielded 43% accuracy, with less variance in the output scores. This was a good sign that we were moving in the right direciton for improving the accuracy of our classifiers.</p>
        <p>I also wanted to see if I could use LDA to find a set of important features, </p>
        <img />
        <p>Now that I'd selected an ideal set of features to use, I wanted to benchmark my performance on more than just random forests, that is, are there other models that would do a better job of predicting? Here's a summary of which models I tried, and how they did:</p>
        <ul>
          <li></li>
          <li></li>
          <li></li>
        </ul>


        <h2>Sources</h2>
        <p>All data scraped from Twitter, via the rtweet package.</p>
        <h2>Tools</h2>
        <p><a href="./assets/oidd/bias.rmd">Here</a> is a link to the R notebook I used to do my analysis. The data I collected and exported:</p>
        <ul>
          <li><a href="./assets/oidd/cleaned_geocoded_trip.csv">Cleaned, geocoded Tripadvisor data</a></li>
        </ul>
    </div>
  </body>
</html>
